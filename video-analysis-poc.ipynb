{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bf47125-ef04-4e27-b784-ad8975e0994c",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bb5dee-43af-4066-9ddf-9fcbd3b30a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install yt-dlp\n",
    "!pip install -q git+https://github.com/openai/whisper.git\n",
    "!pip install ffmpeg-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e273b29-6ac8-4929-a8f5-ae5f99085b43",
   "metadata": {},
   "source": [
    "**PS**: Don't brew install jupyter, else it'll use the system python (and associated packages) instead of the local env. \n",
    "\n",
    "The recommended way to use run jupyter in a local env is:\n",
    "```\n",
    "    pip install jupyter ipykernel\n",
    "    python -m ipykernel install --user --name=video-analysis-poc --display-name \"Python (video-analysis-poc)\"\n",
    "    jupyter lab\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a9a96a-ee89-4158-98f0-9b4af3f999d6",
   "metadata": {},
   "source": [
    "## Transcribe video from URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e603efc-418a-434d-8a91-173d1da921ac",
   "metadata": {},
   "source": [
    "### Unable to extract filler words\n",
    "* Using whisper doesn't return the filler words in the transcription\n",
    "* Passing in an [initial prompt](https://github.com/openai/whisper/discussions/1174#discussioncomment-5490351) as mentioned in the doesn't help either, even when using a tiny model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8368a1e4-7de4-461c-9fb1-6f4505783920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "model = whisper.load_model(\"tiny.en\") # or choose another model like \"small\", \"medium\", etc.\n",
    "result = model.transcribe(audio=\"test-with-fillers.m4a\", initial_prompt=\"Umm, let me think like, hmm... Okay, here's what I'm, like, thinking.\")\n",
    "\n",
    "# 3. Print or process the transcription\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9337e6b-565e-4da6-ad03-6b6e4f318737",
   "metadata": {},
   "source": [
    "* [Prompting](https://platform.openai.com/docs/guides/speech-to-text/prompting#prompting) doesn't help with the API either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d921c2-e2f0-4b8e-a1f1-04445d431377",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "audio_file= open(\"test-with-fillers.m4a\", \"rb\")\n",
    "transcription = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\", \n",
    "    file=audio_file,\n",
    "    prompt=\"Umm, let me think like, hmm... Okay, here's what I'm, like, thinking.\"\n",
    ")\n",
    "\n",
    "print(transcription.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85a9f20-627e-4898-8dbe-e46d71e57d14",
   "metadata": {},
   "source": [
    "* I may be able to use CrisperWhisper for testing. But it's really slow. Plus can't go to production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e2614e-e4a1-47e4-89b6-52e1edecb24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/nyrahealth/transformers.git@crisper_whisper\n",
    "!pip install datasets transformers accelerate\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "def adjust_pauses_for_hf_pipeline_output(pipeline_output, split_threshold=0.12):\n",
    "    \"\"\"\n",
    "    Adjust pause timings by distributing pauses up to the threshold evenly between adjacent words.\n",
    "    \"\"\"\n",
    "\n",
    "    adjusted_chunks = pipeline_output[\"chunks\"].copy()\n",
    "\n",
    "    for i in range(len(adjusted_chunks) - 1):\n",
    "        current_chunk = adjusted_chunks[i]\n",
    "        next_chunk = adjusted_chunks[i + 1]\n",
    "\n",
    "        current_start, current_end = current_chunk[\"timestamp\"]\n",
    "        next_start, next_end = next_chunk[\"timestamp\"]\n",
    "        pause_duration = next_start - current_end\n",
    "\n",
    "        if pause_duration > 0:\n",
    "            if pause_duration > split_threshold:\n",
    "                distribute = split_threshold / 2\n",
    "            else:\n",
    "                distribute = pause_duration / 2\n",
    "\n",
    "            # Adjust current chunk end time\n",
    "            adjusted_chunks[i][\"timestamp\"] = (current_start, current_end + distribute)\n",
    "\n",
    "            # Adjust next chunk start time\n",
    "            adjusted_chunks[i + 1][\"timestamp\"] = (next_start - distribute, next_end)\n",
    "    pipeline_output[\"chunks\"] = adjusted_chunks\n",
    "\n",
    "    return pipeline_output\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"nyrahealth/CrisperWhisper\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=16,\n",
    "    return_timestamps='word',\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "#dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
    "#sample = dataset[0][\"audio\"]\n",
    "with open(\"test-with-fillers.mp3\", \"rb\") as f:\n",
    "    sample = f.read()\n",
    "hf_pipeline_output = pipe(sample)\n",
    "crisper_whisper_result = adjust_pauses_for_hf_pipeline_output(hf_pipeline_output)\n",
    "print(crisper_whisper_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784a1df6-facd-4dc0-9fec-24cf10313145",
   "metadata": {},
   "source": [
    "It's faster to use an API than run inference locally of this 1.6B parameter model. Let's use Deepgram instead. They give $500 of free credits, have low cost per minute and I was able to transcribe my test file and see the transcript. I know it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bce1237",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deepgram-sdk python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9e036b-b7ac-4ccf-ad1c-171770d92d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "from deepgram import (\n",
    "    DeepgramClient,\n",
    "    PrerecordedOptions,\n",
    "    FileSource,\n",
    ")\n",
    "\n",
    "# Path to the audio file\n",
    "AUDIO_FILE = \"test-with-fillers.mp3\"\n",
    "\n",
    "try:\n",
    "    # STEP 1 Create a Deepgram client using the API key\n",
    "    load_dotenv()\n",
    "    deepgram = DeepgramClient(api_key=os.getenv(\"DEEPGRAM_API_KEY\"))\n",
    "\n",
    "    with open(AUDIO_FILE, \"rb\") as file:\n",
    "        buffer_data = file.read()\n",
    "\n",
    "    payload: FileSource = {\n",
    "        \"buffer\": buffer_data,\n",
    "    }\n",
    "\n",
    "    #STEP 2: Configure Deepgram options for audio analysis\n",
    "    options = PrerecordedOptions(\n",
    "        model=\"nova-2\",\n",
    "        #smart_format=True,\n",
    "        filler_words=True\n",
    "    )\n",
    "\n",
    "    # STEP 3: Call the transcribe_file method with the text payload and options\n",
    "    response = deepgram.listen.rest.v(\"1\").transcribe_file(payload, options)\n",
    "\n",
    "    # STEP 4: Extract the transcription from the response\n",
    "    transcription = response.results.channels[0].alternatives[0]\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Exception: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the quality of a presentation\n",
    "1. Words per minute: \n",
    "  - Slow: < 120 wpm\n",
    "  - Best: 120-160 wpm\n",
    "  - Fast: > 120 wpm\n",
    "2. Average number of filler words per minute: Deepgram is capable of transcribing the following [filler words](https://developers.deepgram.com/docs/filler-words) -\n",
    "    * uh\n",
    "    * um\n",
    "    * mhmm\n",
    "    * mm-mm\n",
    "    * uh-uh\n",
    "    * uh-huh\n",
    "    * nuh-uh\n",
    "3. Repetition: Count or identify instances of repetition of words or phrases.\n",
    "4. Clarity: Analyze sentence structure and word choice for complexity and potential ambiguity. You can use libraries like NLTK and spaCy.\n",
    "5. Structure: Identify transitions between topics/slides. Check the use of introductory phrases and conclusion statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2238cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = transcription.transcript\n",
    "total_duration = transcription.words[-1].end - transcription.words[0].start\n",
    "total_words = len(transcription.words)\n",
    "print(f\"Transcript: {transcript}\\n\")\n",
    "\n",
    "# Calcuate words per minute\n",
    "wpm = total_words / (total_duration / 60)\n",
    "print(f\"Words per minute: {wpm:.2f}\")\n",
    "\n",
    "# Calculate filler words per minute and repetitions\n",
    "filler_words = [\"uh\", \"um\", \"mhmm\", \"mm-mm\", \"uh-uh\", \"uh-huh\", \"nuh-uh\"]\n",
    "total_filler_words = 0\n",
    "repetition_count = 0\n",
    "last_word = None\n",
    "for word in transcript.split():\n",
    "    total_filler_words += 1 if word in filler_words else 0\n",
    "\n",
    "    if word == last_word:\n",
    "        repetition_count += 1\n",
    "    last_word = word\n",
    "repetitions_per_minute = repetition_count / (total_duration / 60)\n",
    "print(f\"Total repetitions: {repetition_count}. Repetitions per minute: {repetitions_per_minute:.2f}\")\n",
    "filler_wpm = total_filler_words / (total_duration / 60)\n",
    "print(f\"Total filler words: {total_filler_words}. Filler words per minute: {filler_wpm:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (video-analysis-poc)",
   "language": "python",
   "name": "video-analysis-poc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
